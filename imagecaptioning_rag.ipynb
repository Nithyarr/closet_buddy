{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings #for converting chunks into embeddings\n",
    "from langchain_chroma import Chroma #database for stroring the embeddings\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "from langchain_community.document_loaders import ImageCaptionLoader\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "# Dictionary to track temporary directories\n",
    "temp_dirs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2doc(img_path):\n",
    "    \n",
    "    # Create an ImageCaptionLoader instance\n",
    "    loader = ImageCaptionLoader(img_path)\n",
    "    # Load the caption as a document\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split the document into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Create a temporary directory\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    temp_dirs.append(temp_dir)  # Track temp directory for later cleanup\n",
    "    \n",
    "    # Create a new Chroma vectorstore\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=splits, \n",
    "        embedding=OpenAIEmbeddings(), \n",
    "        persist_directory=temp_dir\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(k=1)\n",
    "    \n",
    "    return retriever, temp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fashiondb(dir):\n",
    "    \"\"\"\n",
    "    dir is the directory of the vector DB\n",
    "    \"\"\"\n",
    "    embeddings_used = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vectorDB = Chroma(persist_directory=dir,embedding_function=embeddings_used)\n",
    "    retriever = vectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_temp_dir(temp_dir):\n",
    "    try:\n",
    "        shutil.rmtree(temp_dir)\n",
    "        #print(f\"Temporary directory {temp_dir} cleaned up successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to clean up temporary directory {temp_dir}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\OneDrive\\Desktop\\final\\closet_care_buddy-main\\chroma_db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dir = os.getcwd()\n",
    "db_dir = os.path.join(dir,\"chroma_db\")\n",
    "print(db_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def textGeneration_langChain_RAG(img_path):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an experienced clothing sylist. \"\n",
    "        \"Use the following pieces of retrieved context to answer. \"\n",
    "        \"Describe the piece of clothing in this picture. \"\n",
    "        \"Use two sentence maximum and be as detailed as possible yet concise. \"\n",
    "        \"Include the clothing syle (i.e. bohemian, casual, classic, sporty, preppy). \"\n",
    "        \"Include how new or worn the item looks to be. \"\n",
    "        \"Be confident avoid using the word 'likely'. \"\n",
    "        \"\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    caption_retriever,temp_dir = img2doc(img_path)\n",
    "    #caption_retriever = fashiondb(db_dir)\n",
    "\n",
    "    # initialize the ensemble retriever\n",
    "    #ensemble_retriever = EnsembleRetriever(\n",
    "    #retrievers=[caption_retriever, reference_retriever], weights=[0.8, 0.2]\n",
    "    #)\n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag_chain = create_retrieval_chain(caption_retriever, question_answer_chain)\n",
    "\n",
    "    response = rag_chain.invoke({\"input\": \"Describe the piece of clothing in this picture\", \"context\": caption_retriever})\n",
    "\n",
    "    #rag_chain = prompt | llm | StrOutputParser()\n",
    "    cleanup_temp_dir(temp_dir)\n",
    "\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# # Load .env variables\n",
    "# load_dotenv()\n",
    "\n",
    "# # Retrieve API key from environment\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# # Initialize ChatOpenAI\n",
    "# llm = ChatOpenAI(api_key=api_key, model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# res = textGeneration_langChain_RAG(\"static/imgs/last_capture_demo.png\")\n",
    "# print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to clean up temporary directory C:\\Users\\admin\\AppData\\Local\\Temp\\tmpfirmekhx: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\admin\\\\AppData\\\\Local\\\\Temp\\\\tmpfirmekhx\\\\308938ce-2f02-4f43-8832-591075bcfd78\\\\data_level0.bin'\n",
      "The piece of clothing is a vibrant green and yellow sari, showcasing a traditional and elegant style. The fabric appears fresh and well-maintained, indicating it is relatively new.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load .env variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API key from environment\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Ensure that the API key is loaded properly\n",
    "if not api_key:\n",
    "    print(\"API key not found. Please check your .env file.\")\n",
    "else:\n",
    "    # Initialize ChatOpenAI\n",
    "    llm = ChatOpenAI(api_key=api_key, model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    # Assuming textGeneration_langChain_RAG is defined elsewhere\n",
    "    res = textGeneration_langChain_RAG(\"static/imgs/sari.png\")\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m()\n\u001b[0;32m      3\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mimages\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdall-e-3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerate_picture_prompt()\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      9\u001b[0m   )\n\u001b[0;32m     11\u001b[0m image_url \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39murl\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=\"generate_picture_prompt()\",\n",
    "    size=\"1024x1024\",\n",
    "    quality=\"standard\",\n",
    "    n=1,\n",
    "  )\n",
    "\n",
    "image_url = response.data[0].url\n",
    "print(image_url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
